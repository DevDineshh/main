# ğŸ“š RAG Kowledgebase Search & Summarization â€” README

## ğŸš€ Overview

This project implements a **Retrieval-Augmented Generation (RAG)** system that retrieves information from Wikipedia, performs hybrid search, and generates summaries using a Large Language Model.

It demonstrates a complete pipeline including:

* Knowledge ingestion
* Hybrid retrieval (BM25 + embeddings)
* LLM summarization
* FastAPI backend
* Streamlit frontend
* Evaluation using ROUGE
* Experiment logging in JSON

This project is ideal for learning, experimentation, and showcasing real-world RAG architecture.

---

## ğŸ—ï¸ Project Structure

```
rag/
â”‚
â”œâ”€â”€ data/                 # Vectordatabase 
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ retrieval.py      # BM25 + hybrid search
â”‚   â”œâ”€â”€ llm.py            # Summarization logic
â”‚   â”œâ”€â”€ embedding.py      # Embedding utilities
â”‚   â”œâ”€â”€ text_preprocessing.py
â”‚   â””â”€â”€ wikip.py          # Wikipedia loader
â”‚
â”œâ”€â”€ test/                 # Saved experiment results (JSON)
â”‚
â”œâ”€â”€ main.py               # FastAPI backend
â”œâ”€â”€ frontend.py           # Streamlit UI
â”œâ”€â”€ eval_retrieval.py     # Evaluation script
â”œâ”€â”€ knowledgebase.py      # Chunk generation pipeline
â”œâ”€â”€ requirements.txt      # Dependency Packages
â””â”€â”€ README.md
```

---

## ğŸ§  System Architecture

```
User â†’ Streamlit â†’ FastAPI â†’ Retriever â†’ LLM â†’ Response
                          â†“
                    Evaluation Logs
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone repository

```
git clone <your_repo_url>
cd rag
```

---

### 2ï¸âƒ£ Create virtual environment

Windows:

```
python -m venv env
env\Scripts\activate
```

macOS/Linux:

```
python -m venv env
source env/bin/activate
```

---

### 3ï¸âƒ£ Install dependencies

```
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Configure environment variables

Create `.env`:

```
GROQ_API_KEY=your_api_key_here
```

---

## ğŸ“¥ Build Knowledge Base

Before running queries, generate document chunks:

```
python knowledgebase.py
```

This will:

* Load Wikipedia pages
* Clean text
* Split into chunks
* Generate embeddings
* Save data in `data/`

---

## â–¶ï¸ Run the System

### Start backend

```
uvicorn main:app --reload
```

Runs at:

```
http://localhost:8000
```

---

### Start frontend

```
streamlit run frontend.py
```

Open:

```
http://localhost:8501
```

---

## ğŸ” Workflow

1. User enters a query
2. Backend performs hybrid retrieval
3. Relevant chunks are fetched
4. LLM generates summary
5. Results displayed in UI
6. Outputs optionally saved for evaluation

---

## ğŸ§ª Evaluation

Evaluate retrieval and summarization quality.

### Run evaluation

```
python eval_retrieval.py
```

Outputs include:

* ROUGE-1
* ROUGE-2
* ROUGE-L
* Average metrics

---

## ğŸ“‚ Test / Results Folder

The `test/` folder stores experiment outputs in JSON format.

Examples:

```
test/
â”œâ”€â”€ test_data.json
â”œâ”€â”€ evaluation.json
â””â”€â”€ run_logs.json
```

Useful for:

* Tracking experiments
* Comparing model performance
* Debugging retrieval

---

## ğŸ“„ Test Data Format

```
test/test_data.json
```

Example:

```
[
  {
    "query": "tell about python",
    "reference_summary": "Python is a programming language..."
  }
]
```

---

## âœ¨ Features

* Hybrid search (keyword + semantic)
* Wikipedia ingestion pipeline
* Chunked knowledge base
* LLM summarization
* FastAPI API
* Streamlit interface
* ROUGE evaluation
* Experiment logging
* Modular architecture

---

## ğŸ›  Troubleshooting

### â— Test file not found

Create:

```
data/test_data.json
```

---

### â— No documents retrieved

Rebuild chunks:

```
python knowledgebase.py
```

---

### â— API key error

Check `.env` file.

---

### â— Streamlit closes immediately

Ensure backend is running first.

---

## ğŸ“Š Example Output

```
Evaluating summarization...

Case 1: tell about python
âœ” Done

=== Average ROUGE Scores ===
ROUGE1: 0.519
ROUGE2: 0.231
ROUGEL: 0.294
```

---

## ğŸ”® Future Improvements

* Add vector database (FAISS / Pinecone / pgvector)
* Reranking models
* Query caching
* Streaming responses
* Experiment tracking dashboards
* Docker deployment
* Monitoring and logging
* Multi-document QA

---

## ğŸ¤ Contributing

1. Fork repository
2. Create branch
3. Submit pull request

---

## ğŸ‘¨â€ğŸ’» Author

Built for experimentation with Retrieval-Augmented Generation pipelines.

---

## ğŸ“œ License

MIT License â€” free to use and modify.
